--- 
layout: post 
title:  "La rivoluzione dell’informazione" 
date:   2015-08-02 16:47:00 
categories: books
---

## <a name="93-105">[93-105](#93-105)</a>
>È perciò molto difficile trovare un punto di partenza univoco e incontroverso. Il padre della teoria dell’informazione, Claude Shannon, fu molto cauto su questo tema: Sono stati suggeriti vari significati della parola informazione da autori differenti nell’ambito generale della teoria dell’informazione. È probabile che un certo numero di tali significati risulterà sufficientemente utile in talune applicazioni da meritare un approfondimento e un’attenzione costante. È difficile al contempo che un concetto unico di informazione renda conto in modo soddisfacente delle sue numerose possibili applicazioni in questo ambito generale.1 Warren Weaver, uno dei pionieri della traduzione automatica e coautore con Shannon della Teoria matematica della comunicazione, propose un’analisi tripartita dell’informazione in termini di: 1)  problemi tecnici concernenti la quantificazione dell’informazione e affrontati con la teoria di Shannon; 2)  problemi semantici relativi al significato e alla verità; 3)  problemi da lui definiti di influenza, attinenti all’impatto e all’effettività dell’informazione sul comportamento umano, che riteneva dovessero avere un ruolo parimenti importante.

Pag. 7.

## <a name="126-29">[126-29](#126-29)</a>
>altre cronologie hanno in comune è di essere storiche, nel senso preciso in cui esse dipendono tutte dallo sviluppo di sistemi per registrare eventi e, quindi, per accumulare e trasmettere informazioni per il futuro. Senza tali sistemi non vi sarebbe storia, cosicché storia è in realtà sinonimo di età dell’informazione, dal momento che la preistoria è quell’età dell’evoluzione umana che precede la disponibilità di sistemi di registrazione.

Pag. 9.

## <a name="155-58">[155-58](#155-58)</a>
>tutti i paesi membri del G7 (Canada, Francia, Germania, Giappone, Gran Bretagna, Italia e Stati Uniti) si definiscono come società dell’informazione in quanto, in ciascuno di essi, almeno il 70 per cento del PIL dipende da beni intangibili, che concernono l’informazione, e non da beni materiali, che sono il prodotto fisico dei settori agricolo e manifatturiero. Il loro funzionamento e la loro crescita richiedono e generano immense quantità di dati, più di quanti l’umanità abbia mai conosciuto nella sua intera storia.

Pag. 11.

## <a name="241-45">[241-45](#241-45)</a>
>Un buon modo per illustrarne il senso è richiamare la distinzione tra applicazioni che migliorano e applicazioni che aumentano. Le applicazioni che apportano miglioramenti, come pacemaker, occhiali o arti artificiali, devono avere interfacce che permettono loro di essere fissate in modo ergonomico al corpo dell’utente. È l’origine dell’idea di cyborg. Le applicazioni che aumentano hanno invece interfacce che consentono di mettere in comunicazione differenti mondi possibili.

Pag. 16.

## <a name="250-56">[250-56](#250-56)</a>
>Le ICT non sono applicazioni che migliorano o aumentano nel senso appena delineato. Sono dispositivi che comportano trasformazioni radicali, dal momento che costruiscono ambienti in cui l’utente è in grado di entrare tramite porte di accesso (possibilmente amichevoli), sperimentando una sorta di iniziazione. Non vi è un termine per indicare questa nuova forma radicale di costruzione, cosicché possiamo usare il neologismo riontologizzare per fare riferimento al fatto che tale forma non si limita solamente a configurare, costruire o strutturare un sistema (come una società, un’auto o un artefatto) in modo nuovo, ma fondamentalmente comporta la trasformazione della sua natura intrinseca, vale a dire della sua ontologia. In tal senso, le ICT non stanno soltanto ricostruendo il nostro mondo: lo stanno riontologizzando.

Pag. 17.

## <a name="258-62">[258-62](#258-62)</a>
>Douglas Engelbart una volta mi ha raccontato che, mentre stava perfezionando la sua più celebre invenzione, il mouse, aveva sperimentato la possibilità di collocarlo al di sotto del tavolo, per poterlo controllare con il ginocchio, lasciando così libere le mani dell’utente. L’idea di partenza era quella della dattilografa, che ha bisogno di entrambe le mani per scrivere. Oggi ci capita spesso di tastare schermi che a volte non sono interattivi. L’interazione essere umano-computer è una relazione simmetrica, di mutua trasformazione.

Pag. 17.
282	Essere è “essere interagibile”, anche se l’interazione è solo indiretta.

Pag. 19.

## <a name="294-95">[294-95](#294-95)</a>
>Le generazioni future erediteranno beni digitali che vorranno possedere.

Pag. 20.

## <a name="311-14">[311-14](#311-14)</a>
>nel 2008 il giocatore di età media aveva trentacinque anni e giocava già da tredici anni, l’età media dei più assidui acquirenti di giochi era di quarant’anni e il 26 per cento degli americani di più di cinquant’anni giocava ai videogiochi, con un incremento del 9 per cento dal 1999. Che cosa pensate che faranno, quando andranno in pensione?

Pag. 21.
347	“L’infosfera sta progressivamente assorbendo ogni altro spazio”.

Pag. 23.

## <a name="350-57">[350-57](#350-57)</a>
>Un buon esempio (ma non l’unico) è offerto dai tag RFID (radio frequency identification), che possono immagazzinare e reperire, a distanza, dati da altri oggetti e conferire loro un’identità unica, come un codice a barre. I tag possono misurare 0,4 mm2 e sono più sottili della carta. È sufficiente incorporare questo piccolo microchip in qualsiasi oggetto, inclusi esseri umani e animali, per creare IT-enti. Non si tratta di fantascienza. Secondo un rapporto della società di ricerche di mercato In-Stat, la produzione mondiale di RFID è aumentata più di venticinque volte tra il 2005 e il 2010 e raggiungerà il numero di 33 miliardi. Immaginiamo di collegare insieme questi 33 miliardi di IT-enti con le centinaia di milioni di computer, lettori DVD, iPod, iPad e altri dispositivi ICT disponibili e ci renderemo conto immediatamente che l’infosfera non è più là ma qui ed è qui per restarci.

Pag. 23.

## <a name="362-66">[362-66](#362-66)</a>
>Nelle società avanzate dell’informazione, ciò che oggi sperimentiamo come mondo offline è destinato a diventare un ambiente totalmente interattivo e più capace di rispondere, costituito da processi informativi A2A (anything to anything)8, wireless, pervasivi e distribuiti, che operano A4A (anywhere for anytime)9 in tempo reale.

Pag. 24.

## <a name="386-88">[386-88](#386-88)</a>
>Tale gap non è riconducibile alla distanza che esiste tra paesi industrializzati e in via di sviluppo, ma attraverserà le società dall’interno. Stiamo preparando il terreno per le baraccopoli digitali del domani.

Pag. 26.

## <a name="450-52">[450-52](#450-52)</a>
>Il modo in cui i dati giungono ad avere un determinato significato e funzionano all’interno di un sistema di segni (semiotico) come un linguaggio naturale è una delle questioni più complesse in semantica, conosciuta come problema del fondamento simbolico.

Pag. 30.

## <a name="473-77">[473-77](#473-77)</a>
>Il punto è che un’autentica e totale cancellazione di tutti i dati può ottenersi soltanto tramite l’eliminazione di tutte le possibili differenze. Ciò chiarisce perché un dato è riducibile, in ultima analisi, a una mancanza di uniformità. Donald Mac-Crimmon MacKay ha posto in evidenza questo aspetto importante, affermando che «l’informazione è una distinzione che fa differenza». In questo fu seguito da Gregory Bateson, il cui slogan è più conosciuto sebbene sia meno accurato: «In effetti, ciò che intendiamo per informazione – l’unità elementare dell’informazione – è una differenza che fa differenza».

Pag. 31.

## <a name="477-83">[477-83](#477-83)</a>
>In termini più formali, la definizione generale di dato, secondo l’interpretazione diaforica (diaphora è la parola greca per differenza), è la seguente: Dd) dato = def.x è distinto da y, dove x e y sono due variabili non interpretate e la relazione “è distinto”, così come il suo dominio, sono aperte a ulteriore interpretazione. La definizione di dato può trovare applicazione in tre modi principali. 1) In primo luogo, i dati possono essere mancanze di uniformità nel mondo reale.

Pag. 32.

## <a name="492-94">[492-94](#492-94)</a>
>i dati possono essere mancanze di uniformità tra (la percezione di) almeno due stati fisici di un sistema o segnali. Esempi possono essere offerti dalla carica più alta o più bassa di una batteria, dal segnale elettrico variabile in una conversazione telefonica o dal punto e dalla linea nell’alfabeto Morse.

Pag. 33.

## <a name="495-98">[495-98](#495-98)</a>
>3) Infine, i dati possono essere mancanze di uniformità tra due simboli, come tra le lettere B e P nell’alfabeto latino. In base all’interpretazione data, i dedomena in (1) possono essere identici con, o ciò che rende possibili, i segnali in (2); e i segnali in (2) sono ciò che consente di codificare i simboli in (3).

Pag. 33.

## <a name="498-501">[498-501](#498-501)</a>
>Il fatto che le informazioni dipendano dal sussistere di dati sintatticamente ben formati e che questi, a loro volta, dipendano da differenze variamente implementabili dal punto di vista fisico spiega perché le informazioni possano essere facilmente disgiunte dal loro supporto. Il formato concreto, il mezzo di comunicazione e il linguaggio in cui i dati e quindi le informazioni sono codificate, è spesso irrilevante e privo di interesse.

Pag. 33.

## <a name="514-19">[514-19](#514-19)</a>
>I computer sono di regola considerati sistemi di informazione digitale o discreta, ma come Turing stesso ha messo in evidenza: I calcolatori digitali […] possono essere classificati tra le «macchine a stati discreti», cioè tra quelle che si muovono a salti o scatti improvvisi da uno stato ben definito a un altro. Questi stati sono abbastanza differenti tra loro perché si possa ignorare la possibilità di confusione tra di essi. Strettamente parlando non esistono macchine di questo genere: in realtà ogni cosa si muove con continuità. Ma ci sono molti tipi di macchine che possono vantaggiosamente essere viste come macchine a stati discreti.

Pag. 34.

## <a name="624-28">[624-28](#624-28)</a>
>Il sistema binario di codificazione dei dati possiede almeno tre vantaggi. In primo luogo, i bit sono rappresentati con pari efficacia in termini semantici (significato vero/falso), logico-matematici (che sta per 1/0) e fisici (transistor = on/off; cancello = aperto/chiuso; circuito elettrico = basso/alto voltaggio; disco o nastro = magnetizzato/non magnetizzato; cd = presenza/assenza di “buchi” ecc.): ciò costituisce il terreno comune su cui possono convergere semantica, logica matematica, fisica e ingegneria dei circuiti e teoria dell’informazione.

Pag. 41.

## <a name="628-33">[628-33](#628-33)</a>
>Il secondo vantaggio consiste nel fatto che diviene dunque possibile costruire macchine che riconoscono bit dal punto di vista fisico, operano logicamente sulla base di tale riconoscimento e, di conseguenza, manipolano dati con modalità che reputiamo dotate di significato. Si tratta di un aspetto fondamentale. La sola scintilla di intelligenza che si può oggi accordare pacificamente a un computer riguarda la capacità dei suoi dispositivi e circuiti di discriminare tra dati binari. Se un computer percepisce qualcosa, non è altro che la differenza tra il basso o alto voltaggio al quale i suoi circuiti sono programmati per agire.

Pag. 41.

## <a name="634-38">[634-38](#634-38)</a>
>Infine, dal momento che i dati digitali hanno di regola due soli stati, tale variazione discreta comporta che un computer difficilmente possa confondersi al riguardo di ciò che deve essere processato, a differenza di una macchina analogica che può non di rado funzionare in modo insoddisfacente o impreciso. E, soprattutto, una macchina digitale può riconoscere se taluni dati sono incompleti e quindi ricostituire, attraverso calcoli matematici, i dati che siano andati persi, se vi è qualcosa che, per così dire, suona strano nella quantità di bit che sta processando.

Pag. 42.

## <a name="756-62">[756-62](#756-62)</a>
>In termini più generali, siamo abituati a considerare che le informazioni possono essere codificate, trasmesse e immagazzinate in specifiche quantità, come segnali fisici. Consideriamo inoltre che le informazioni si sommano, come i biscotti e le monete: se do l’informazione a + l’informazione b, avrò dato l’informazione a + b. Le informazioni, infine, non sono mai “negative”: come le probabilità e i tassi di interesse non vanno mai sotto lo zero, a differenza del mio conto corrente o della temperatura a Oxford. Torniamo al nostro esempio: quando Giovanni pone al suo vicino una domanda, lo scenario peggiore che gli si possa presentare è di non ricevere alcuna risposta, o di ricevere la risposta sbagliata; in entrambi i casi Giovanni resterebbe senza alcuna nuova informazione.

Pag. 50.

## <a name="762-67">[762-67](#762-67)</a>
>Queste e altre proprietà quantitative dell’informazione sono analizzate con successo da molteplici approcci matematici. La teoria matematica della comunicazione (TMC) è certamente quello più importante, influente e ampiamente conosciuto. È a Claude Shannon e al suo fondamentale lavoro che si deve il nome di questa branca della teoria delle probabilità. Shannon è stato il pioniere degli studi matematici dell’informazione e colui che ha ottenuto la maggior parte dei principali risultati della disciplina, sebbene avesse riconosciuto l’importanza di lavori precedenti realizzati da altri ricercatori e colleghi presso i laboratori Bell. Dopo Shannon la TMC è diventata nota come teoria dell’informazione, e oggi Shannon ne è considerato il padre:

Pag. 50.

## <a name="826-28">[826-28](#826-28)</a>
>L’idea di fondo è che l’informazione possa essere quantificata in termini di riduzione del deficit di dati (l’incertezza di Shannon). Una moneta produce un solo bit di informazione, due monete ne producono due, tre monete tre e così via.

Pag. 54.

## <a name="862-65">[862-65](#862-65)</a>
>Tuttavia la ridondanza non è sempre qualcosa di negativo, perché può contrastare il fraintendimento (dati inviati ma mai ricevuti) e il rumore (dati ricevuti ma mai inviati). Un messaggio + rumore contiene più dati di quanti non ne contenesse il messaggio originale; ma il fine di un processo comunicativo è la fedeltà, la trasmissione accurata del messaggio originale dall’emittente al ricevente, senza incremento di dati.

Pag. 57.

## <a name="871-75">[871-75](#871-75)</a>
>Per la teoria matematica della comunicazione, l’informazione è soltanto la selezione di un simbolo da un insieme di simboli possibili. Ciò implica che la maniera più semplice di afferrare concettualmente in che modo la TMC quantifichi l’informazione è di considerare il numero di domande sì/no richiesto per determinare che cosa la fonte stia comunicando. Una sola “domanda” è sufficiente per determinare l’esito di una moneta, che produce dunque un solo bit di informazione.

Pag. 57.

## <a name="877-79">[877-79](#877-79)</a>
>In primo luogo, la TMC non è una teoria dell’informazione nel senso ordinario della parola, poiché in essa l’informazione ha un significato squisitamente tecnico.

Pag. 58.

## <a name="893-98">[893-98](#893-98)</a>
>In secondo luogo, dal momento che la TMC è una teoria dell’informazione senza significato (non nel senso di essere priva di significato ma nel senso di non fare ricorso ad esso) e alla luce del fatto che l’informazione meno il significato è uguale ai dati, l’espressione teoria matematica della comunicazione di dati è una descrizione molto più appropriata di questa branca della teoria delle probabilità rispetto a teoria dell’informazione. Non è una mera questione di etichette. L’informazione, in quanto contenuto semantico (di cui si dirà tra breve), può essere descritta anche come dati + domande.

Pag. 59.

## <a name="905-8">[905-8](#905-8)</a>
>La parola informazione non è riferita tanto a ciò che in effetti diciamo quanto piuttosto a ciò che potremmo dire. La teoria matematica della comunicazione concerne ciò che veicola l’informazione, simboli e segnali, e non l’informazione stessa. In altri termini, l’informazione è la misura della nostra libertà di scelta nel selezionare un messaggio.

Pag. 59.

## <a name="910-12">[910-12](#910-12)</a>
>TMC è generalmente descritta come lo studio dell’informazione a livello sintattico. E poiché i computer sono dispositivi sintattici, comprendiamo il motivo per cui la TMC può essere applicata con così grande successo alle ICT.

Pag. 60.

## <a name="913-26">[913-26](#913-26)</a>
>L’informazione nel senso di Shannon è nota anche come entropia. Sembra che dobbiamo questa denominazione, che genera così tanta confusione, a John von Neumann, uno dei più brillanti scienziati del ventesimo secolo, che l’aveva raccomandata a Shannon in questi termini: Dovresti chiamarla entropia per due motivi: in primo luogo, la funzione è già utilizzata nella termodinamica con lo stesso nome; in secondo luogo e cosa più importante, la maggior parte delle persone non sa che cosa realmente sia l’entropia, così se usi la parola entropia in una discussione finirai per avere sempre ragione. Von Neumann ha avuto ragione su entrambi i punti, sfortunatamente. Se assumiamo il caso ideale di un canale di comunicazione privo di rumore, l’entropia è la misura di tre quantità equivalenti: a)   la quantità media di informazione per simbolo prodotta dall’emittente, ovvero b)   la corrispondente quantità media di deficit di dati (l’incertezza di Shannon) che il ricevente ha prima di conoscere il risultato prodotto dall’emittente, ovvero c)   la corrispondente potenzialità informativa della stessa fonte, vale a dire, la sua entropia informativa. L’entropia può parimenti indicare (a) o (b), poiché, selezionando un determinato alfabeto, l’emittente crea automaticamente nel ricevente un deficit di dati (incertezza), che può dunque essere soddisfatto (risolto) in vario grado dal contenuto informativo.

Pag. 60.

## <a name="931-36">[931-36](#931-36)</a>
>Per quanto riguarda (c), la TMC concepisce l’informazione in termini di quantità fisica, come la massa o l’energia, e l’affinità tra la sua analisi dell’informazione e la formulazione del concetto di entropia nella meccanica statistica è già stata discussa da Shannon. Il concetto informazionale di entropia e quello della termodinamica sono posti in relazione attraverso i concetti di probabilità e di casualità. Casualità è preferibile a disordine dal momento che il primo è un concetto sintattico, mentre il secondo ha una forte connotazione semantica, è cioè facilmente soggetto a interpretazione, come tentavo di spiegare ai miei genitori da ragazzo.

Pag. 61.

## <a name="936-42">[936-42](#936-42)</a>
>Può anche essere vista come un indicatore della reversibilità: se non vi è modificazione di entropia, allora il processo è reversibile. Un messaggio molto strutturato e perfettamente organizzato contiene un livello basso di entropia o casualità, cioè una minore informazione nel senso di Shannon, e quindi causa un deficit di dati più ridotto, che può essere vicino a zero (come nel caso del corvo). Al contrario, quanto più elevata è la casualità potenziale dei simboli nell’alfabeto, tanti più bit di informazione possono essere prodotti dal dispositivo. L’entropia assume il suo valore massimo nel caso estremo di distribuzione uniforme: ciò significa che un bicchiere d’acqua con un cubetto di ghiaccio ha un livello minore di entropia rispetto a quella di un bicchiere d’acqua una volta che il cubetto si sia sciolto.

Pag. 62.

## <a name="1030-38">[1030-38](#1030-38)</a>
>Oggi molti ricercatori convengono sul fatto che la TMC fornisca un vincolo rigoroso nei confronti di ogni ulteriore teorizzazione che investa gli aspetti semantici e pragmatici dell’informazione. Il disaccordo concerne la questione fondamentale della forza di tale vincolo. A un estremo dello spettro, si suppone che la teoria dell’informazione semantica fattuale sia vincolata molto strettamente, se non addirittura sovradeterminata, dalla TMC, come l’ingegneria meccanica lo è in un certo senso dalla fisica newtoniana. L’interpretazione ottimistica di Weaver del lavoro di Shannon, incontrata nell’Introduzione, ne è un tipico esempio. All’estremo opposto, vi è l’ipotesi che la teoria dell’informazione semantica fattuale sia vincolata solo debolmente, se non addirittura totalmente sottodeterminata, dalla TMC, nello stesso modo in cui sia il tennis sia il calcio sono entrambi in un certo senso vincolati dalla fisica newtoniana, vale a dire nell’accezione meno interessante e pertinente, e quindi più trascurabile, del termine.

Pag. 68.

## <a name="1051-55">[1051-55](#1051-55)</a>
>Il PRI si riferisce alla relazione inversa che sussiste tra la probabilità di p, dove p può essere una proposizione, una frase di un certo linguaggio, un evento, una situazione o un mondo possibile, e la quantità di informazione veicolata da p. Il pri afferma che l’informazione procede di pari passo con l’imprevedibilità (il fattore sorpresa di Shannon). Ricordiamoci del fatto che il corvo di Poe, in quanto fonte unaria, non forniva alcuna informazione, dal momento che le sue risposte erano interamente prevedibili.

Pag. 69.

## <a name="1057-62">[1057-62](#1057-62)</a>
>Karl Popper è spesso riconosciuto come il primo ad aver formulato esplicitamente il PRI. Tuttavia soltanto dopo la TMC di Shannon sono stati fatti tentativi sistematici di sviluppare un calcolo formale che coinvolgesse tale principio. La TMC definisce l’informazione in termini di probabilità. In una simile linea di pensiero, l’approccio probabilistico all’informazione semantica definisce l’informazione in p nei termini della relazione inversa che esiste tra l’informazione e la probabilità di p. Questo approccio è stato inizialmente proposto da Yehoushua Bar-Hillel e Rudolf Carnap.

Pag. 69.

## <a name="1064-66">[1064-66](#1064-66)</a>
>Lo scandalo della deduzione In base al PRI, quanto più probabile o possibile è p, tanto meno è informativo.

Pag. 70.

## <a name="1068-87">[1068-87](#1068-87)</a>
>pensiamo a cosa accade quando la probabilità di p è massima, cioè quando P(p) = 1. In tal caso p è equivalente a una tautologia, ossia qualcosa che è sempre vero. Le tautologie sono note per essere non informative: Giovanni riceverebbe dati ma nessuna informazione semantica se qualcuno gli dicesse che una nuova batteria sarà o non sarà disponibile in futuro. Di nuovo, ciò appare molto ragionevole. Tuttavia, nella logica classica, una conclusione Q è deducibile da un insieme finito di premesse P1, …, Pn se e solo se il condizionale [P1 e P2, …, Pn implica Q] è una tautologia. In questa prospettiva, dal momento che le tautologie non veicolano alcuna informazione, nessuna inferenza logica può generare un incremento di informazione, cosicché anche le deduzioni logiche, che possono essere analizzate in termini di processi tautologici, non sono in grado di fornire informazioni. Dunque, se si identifica l’informazione semantica veicolata da una frase con l’insieme di tutti i mondi possibili o le circostanze che esclude, è possibile riconoscere che, in ogni valida deduzione, l’informazione veicolata dalla conclusione deve essere già contenuta nell’informazione veicolata dalla congiunzione delle premesse. Questo è ciò che di regola si intende allorché si definiscono le tautologie e le inferenze come analitiche. Ma allora la logica e la matematica sarebbero totalmente non informative. Tale conclusione controintuitiva è nota come lo scandalo della deduzione, che il filosofo e logico Jaakko Hintikka descrive in questi termini: C.D. Broad ha definito i problemi irrisolti che concernono l’induzione uno scandalo filosofico. Mi sembra che oltre allo scandalo dell’induzione vi sia uno scandalo altrettanto inquietante: quello della deduzione. La sua urgenza può essere facilmente compresa da qualsiasi matricola intelligente che, sapendo che il ragionamento deduttivo è “tautologico” o “analitico” e che le verità logiche non hanno “contenuto empirico” e non possono essere utilizzate per formulare “asserzioni fattuali”, si chieda: in quale altro senso, allora, il ragionamento deduttivo ci fornisce nuove informazioni? Non è infatti ovvio che debba esserci questo altro senso, dato che altrimenti non ci sarebbe ragione di sviluppare la logica e la matematica?

Pag. 70.

## <a name="1092-99">[1092-99](#1092-99)</a>
>Logica e matematica producono un incremento di informazione ma soltanto per menti limitate come le nostre, che non sono in grado di percepire che la conclusione è già implicita nelle premesse. Questo approccio non è soddisfacente, dal momento che non riesce a spiegare perché, se la conclusione di un’argomentazione deduttiva è sempre “contenuta” nelle premesse, il ragionamento deduttivo è generalmente percepito come qualcosa di particolarmente importante a fini scientifici. Se tutti i teoremi fossero contenuti negli assiomi di una teoria, le scoperte matematiche sarebbero impossibili. Inoltre è molto difficile, di regola, provare teoremi interessanti in termini di risorse computazionali. Altri approcci hanno mostrato che le deduzioni logico-matematiche classiche sono informative perché la prova della loro validità richiede essenzialmente la (temporanea) introduzione di “informazione virtuale”, la quale è assunta, utilizzata e poi messa da parte, senza lasciare traccia di sé al termine del processo, ma contribuendo fortemente al suo successo.

Pag. 72.

## <a name="1121-24">[1121-24](#1121-24)</a>
>Sebbene il processo sia semplice e alquanto ovvio, è altrettanto chiaro che Giovanni sia fuoriuscito silenziosamente dallo spazio di informazione che possedeva, si sia mosso in uno spazio di informazione virtuale, facendo una quantità considerevole di lavoro essenziale, e abbia quindi fatto ritorno nello spazio di informazione che aveva originariamente, ottenendo la sua conclusione. Se non si presta grande attenzione, il trucco magico è quasi invisibile.

Pag. 74.

## <a name="1131-38">[1131-38](#1131-38)</a>
>Tuttavia, se continuiamo a rendere p sempre meno probabile, raggiungiamo un punto in cui la probabilità di p è in realtà zero, vale a dire p è impossibile o equivale a una contraddizione, ma, in base al PRI, ciò dovrebbe coincidere con il punto in cui p è massimamente informativo. Giovanni riceverebbe dunque la quantità più elevata di informazioni allorché gli fosse detto che la batteria dell’auto è e non è a terra (al contempo e nello stesso senso). Ho chiamato questa ulteriore conclusione controintuitiva il paradosso di Bar-Hillel-Carnap (perché i due filosofi sono stati tra i primi a rendere esplicita l’idea controintuitiva che le contraddizioni siano altamente informative). Dal momento della sua formulazione, il problema è stato riconosciuto come una conseguenza sfortunata, e tuttavia perfettamente corretta e logicamente inevitabile, di ogni teoria quantitativa dell’informazione semantica debole. Debole perché la verità non gioca in essa alcun ruolo.

Pag. 74.

## <a name="1217-54">[1217-54](#1217-54)</a>
>James Clerk Maxwell, il padre della teoria elettromagnetica classica, concepì un esperimento mentale per chiarire ciò che aveva percepito come la natura statistica della seconda legge della termodinamica. Nella sua Teoria del calore invitava il lettore a immaginare il seguente scenario (si veda la Figura 5.2): un contenitore, riempito di gas, è diviso in due parti, A e B; lungo il divisorio vi sono un foro microscopico e un essere, in seguito divenuto noto come demone di Maxwell, che può aprirlo o chiuderlo tramite una porticina; il demone controlla le molecole che si muovono casualmente a differenti velocità; quando si avvicinano alla porticina, il demone la socchiude per consentire alle molecole più veloci della media di passare da A a B e alle molecole meno veloci della media di passare da B ad A. Alla fine il demone distingue tutte le particelle in più veloci (A) e più lente (B), fornendo così un’eccezione alla seconda legge: il rimescolamento si è ridotto senza alcuna immissione di energia. Figura 5.2. Il demone di Maxwell. Ci si rese conto presto che il demone di Maxwell è un dispositivo che utilizza informazioni, in quanto controlla e calcola la traiettoria delle particelle. Se fosse teoreticamente possibile, avremmo identificato un modo logicamente plausibile di servirci dell’informazione per vincere l’entropia fisica, generando lavoro da un sistema a costi energetici più bassi di quelli richiesti dalla seconda legge (si ricordi che la velocità media delle molecole corrisponde alla temperatura, che diminuisce in A e aumenta in B, rendendo possibile una certa quantità di lavoro). Tuttavia la seconda legge della termodinamica appare incontrovertibile. Dunque, dove è il trucco? Nella versione originale di Maxwell, il demone doveva socchiudere la porticina, il che richiedeva una certa energia. A questo aspetto dell’esperimento mentale si può tuttavia ovviare realizzandolo in modo leggermente differente (con porte scorrevoli, dispositivi meccanici ecc.). Come due grandi fisici, Leó Szilárd e Léon Brillouin, hanno compreso, il vero trucco sta nel processo informativo svolto dal demone. Ogni raccolta di informazioni, come il controllo della localizzazione e della velocità delle particelle, richiede energia. Immaginiamo, ad esempio, che il demone usi un raggio di luce per “visualizzare” dove siano le particelle: i fotoni che illuminano le particelle per indicare la loro posizione devono essere stati prodotti da una fonte di energia. Anche se ulteriori miglioramenti nella configurazione del sistema potessero consentire di superare questo limite particolare, vi sarebbe un vincolo ultimo. Una volta che l’informazione sia stata raccolta, il demone deve intraprendere alcune attività per processarla, come calcolare esattamente quando socchiudere la porticina, se intende lavorare in maniera adeguata e ridurre così l’entropia del sistema. Ma l’attività di calcolo, per quanto efficiente, utilizza comunque memoria; il demone deve prima immagazzinare l’informazione, per poterla in seguito manipolare. Pertanto, nella misura in cui il nostro demone continua a operare, l’entropia diminuisce, ma l’immagazzinamento della memoria aumenta. Due informatici sono infine riusciti a esorcizzare il demone. Rolf W. Landauer ha sostenuto, per primo, che ogni manipolazione logicamente irreversibile di informazione causa il rilascio di una certa quantità di calore e quindi genera un corrispondente incremento di entropia nell’ambiente. Successivamente Charles H. Bennet ha dimostrato che la maggior parte dei calcoli può essere fatta in modo reversibile, ma che vi è un’operazione computazionale necessariamente irreversibile: la cancellazione della memoria (si veda il Capitolo 2). Per questo il demone avrà sempre bisogno di energia per cancellare la propria memoria e questa energia è, per così dire, il “conto” che il sistema sotto controllo deve pagare in termini di entropia. La conclusione è che l’informazione sia un fenomeno fisico soggetto alle leggi della termodinamica. O così sembrava fino a poco tempo fa, dal momento che la nostra storia ha un finale aperto: il principio di Landauer non è una legge ed è stato messo in discussione, negli ultimi anni, poiché in realtà presuppone, piuttosto che sostenere, la seconda legge della termodinamica; inoltre si potrebbe avanzare l’ipotesi che sia logicamente plausibile (sebbene fisicamente non possibile, ma questo è il motivo per cui il demone è un esperimento mentale e non un progetto da realizzare) che il demone non abbia bisogno di cancellare la propria memoria. Se nessuna informazione è cancellata, tutti gli altri calcoli possono essere eseguiti in modo tale da risultare reversibili dal punto di vista termodinamico, senza richiedere pertanto né un ulteriore rilascio di calore né un incremento di entropia.

Pag. 80.

## <a name="1263-70">[1263-70](#1263-70)</a>
>Tuttavia gli stati quantistici delle particelle atomiche hanno una natura diversa. Possono essere usati per immagazzinare dati in una sovrapposizione quantistica, definibile ma nondimeno indeterminata, di due stati allo stesso tempo. In chiave metaforica il lettore potrebbe prendere a riferimento quelle figure, rese celebri da Maurits Cornelis Escher, che contengono due interpretazioni al contempo parimenti valide ma incompatibili. O un’altra celebre immagine in cui possiamo vedere alternativamente ma non simultaneamente il volto di una donna anziana o il profilo di una giovane. Il risultato di una tale sovrapposizione di stati è noto come qubit (un bit quantistico). Un qubit è in realtà sia nello stato-0 sia nello stato-1 simultaneamente, sebbene in misura eventualmente differente. È un’unità di informazione oscillante, che precipita invariabilmente nello stato 0 o 1 soltanto quando il suo stato è osservato o misurato.

Pag. 83.

## <a name="1270-73">[1270-73](#1270-73)</a>
>Questo fenomeno fisico di sovrapposizione di stati è consueto in natura, ma è fortemente controintuitivo per il senso comune, dal momento che è difficile afferrare in che senso un qubit possa trovarsi simultaneamente in due stati opposti. Un computer quantistico (CQ) gestisce qubit e per questo motivo, se fossimo in grado di costruirlo, sarebbe straordinariamente potente.

Pag. 83.

## <a name="1281-88">[1281-88](#1281-88)</a>
>Ciò è noto come parallelismo quantistico: avendo davanti a sé l’intera matrice di 8 stati in ogni singola operazione, un CQ può esplorare tutte le possibili soluzioni del problema in un solo passaggio. Più ampio è il registro, più il CQ diventa potente, in modo esponenziale, al punto tale che un CQ con un registro di 64 qubit potrebbe risultare più intelligente di qualsiasi rete di supercomputer. I computer quantistici, se fisicamente implementati, rappresenterebbero un nuovo genere di sistema informativo, alternativo al computer odierno, basato sulla fisica newtoniana. La loro maggiore potenza di calcolo ci obbligherebbe a ripensare la natura e i limiti della complessità computazionale. I CQ non solo renderebbero obsolete le attuali applicazioni nel campo della crittografia, basate su difficoltà di fattorizzazione, ma fornirebbero anche nuovi mezzi per generare crittosistemi assolutamente sicuri e, più generalmente, per trasformare in operazioni banali calcoli statistici di estrema complessità.

Pag. 84.

## <a name="1296-1303">[1296-1303](#1296-1303)</a>
>L’informatica tradizionale è basata sul fatto che le risorse spaziali (localizzazione, memoria, stabilità degli stati fisici ecc.) non rappresentino il problema maggiore, ma che lo sia invece il tempo. L’informatica quantistica può far fronte alle difficoltà collegate al tempo proprie dell’informatica tradizionale (alcuni processi informativi semplicemente richiedono troppo tempo) grazie a uno slittamento. La relazione tra tempo (quanti passaggi) e spazio computazionale (quanta memoria) è invertita: il tempo diviene meno problematico dello spazio, attraverso la trasformazione dei fenomeni quantistici di sovrapposizione, che sono di breve durata e incontrollabili a livello microscopico, in fenomeni quantistici sufficientemente durevoli e controllabili a livello macroscopico da consentire l’implementazione dei processi computazionali. I computer quantistici diventeranno un bene di consumo soltanto se questo passaggio diverrà empiricamente possibile.

Pag. 85.

## <a name="1304-6">[1304-6](#1304-6)</a>
>Invero, secondo taluni ricercatori, potrebbero scoprire che la realtà (ciò che è) sia in sé fatta di informazioni (il bit), il tema del prossimo e ultimo paragrafo.

Pag. 86.

## <a name="1316-23">[1316-23](#1316-23)</a>
>Tutto ciò spiega perché la fisica dell’informazione è coerente con due slogan, celebri tra gli scienziati ed entrambi favorevoli alla natura protofisica dell’informazione. Il primo lo dobbiamo a Norbert Wiener, il padre della cibernetica: «L’informazione è informazione. Non materia o energia. Qualsiasi materialismo che non l’ammetta è destinato a non sopravvivere». L’altro appartiene a John Archibald Wheeler, un eminente fisico, che ha coniato l’espressione it from bit, per indicare che la natura ultima della realtà fisica, “ciò che è”, è informazionale, proviene dai bit. In entrambi i casi la fisica finisce per fare propria una descrizione della natura basata sull’informazione. L’universo è fondamentalmente composto di dati, concepiti come dedomena, vale a dire come insiemi o domini di differenze, invece che essere composto di materia o di energia, con la conseguenza che gli oggetti materiali sono considerati come una complessa manifestazione secondaria.

Pag. 86.

## <a name="1323-29">[1323-29](#1323-29)</a>
>Questa metafisica informazionale può, ma non necessariamente deve, fare propria l’idea molto più controversa per cui l’universo fisico è concepito alla stregua di un gigantesco computer digitale e i processi dinamici sono una tipologia di transizioni negli stati computazionali. La distinzione può sembrare sottile ma è fondamentale. Immaginiamo di descrivere lo stomaco come se fosse un computer (con input, stadi in cui i dati sono processati e output), di contro alla possibilità di considerare che lo stomaco sia in effetti un computer. Se l’universo fisico possa essere veramente e adeguatamente descritto da un modello digitale e computazionale è un quesito differente dal chiedersi se la natura ultima dell’universo fisico sia in sé effettivamente digitale e computazionale.

Pag. 87.

## <a name="1339-48">[1339-48](#1339-48)</a>
>Inoltre, se il mondo fosse un computer, ciò implicherebbe la totale prevedibilità dei suoi sviluppi e resusciterebbe così un altro demone, quello di Laplace. Pierre-Simon Laplace, uno dei padri fondatori dell’astronomia matematica e della statistica, ha avanzato l’idea che se un essere ipotetico (noto come demone di Laplace) potesse avere tutte le informazioni necessarie riguardo alla collocazione esatta nello spazio e nel tempo di ogni atomo dell’universo, allora potrebbe avvalersi delle leggi di Newton per calcolare l’intera storia dell’universo. Questa forma radicale di determinismo era ancora popolare nel diciannovesimo secolo, ma già in quello successivo fu minata alle basi dalla natura manifestamente probabilistica dei fenomeni quantistici. Il cammino della scienza ha fatto sì che a suo fondamento non vi siano più necessità e leggi ma probabilità e vincoli. Oggi in fisica è per lo più accettato che le particelle si comportino in modo non deterministico, seguendo il principio di incertezza. Concordemente all’interpretazione di Copenhagen della meccanica quantistica, che è senz’altro la più condivisa tra i fisici, il determinismo computazionale non è una vera opzione. Il demone di Laplace è un fantasma e la fisica digitale segue la sua stessa sorte.

Pag. 88.

## <a name="1350-54">[1350-54](#1350-54)</a>
>Sulla scorta di Wiener e Wheeler si potrebbe interpretare la realtà come costituita da informazioni, vale a dire da entità strutturali, indipendenti dalla mente, che sono insiemi coerenti di dati, compresi a loro volta come punti relazionali di mancanza di uniformità. Questa realtà strutturale consente o accoglie certi costrutti e resiste a, o ne impedisce, altri, in ragione della natura dei sistemi informativi (quali, ad esempio, inforg come noi), che la abitano, e delle interazioni che intrattiene con loro. Se un approccio informazionale alla natura della realtà è soddisfacente, che cosa potrebbe dirci della natura della vita?

Pag. 88.

## <a name="1371-78">[1371-78](#1371-78)</a>
>vi sono tre modi principali in cui è possibile parlare di informazione: a)  informazione come realtà: ad esempio le impronte digitali o gli anelli concentrici nel tronco degli alberi; Figura 6.1. L’informazione biologica. b)   informazione per la realtà: ad esempio comandi, algoritmi o ricette; c)   informazione sulla realtà, cioè con valore epistemico: ad esempio orari dei treni, mappe, voci nell’enciclopedia.

Pag. 90.

## <a name="1378-83">[1378-83](#1378-83)</a>
>Qualcosa può essere qualificato come informazione in più di un senso, in ragione del contesto. Ad esempio l’iride di una persona può essere un’istanza di informazione come realtà (la struttura della membrana di un occhio), che offre informazioni per la realtà (ad esempio come strumento biometrico per l’apertura di una porta previa verifica dell’identità di una persona) o sulla realtà (ad esempio l’identità di una persona). Occorre tuttavia essere chiari circa il senso in cui l’informazione è utilizzata in ciascun caso: (a) fisico, (b) istruttivo, (c) semantico. Sfortunatamente l’informazione biologica è spesso impiegata in tutti e tre i sensi allo stesso tempo.

Pag. 90.

## <a name="1402-8">[1402-8](#1402-8)</a>
>Nel 1944, in un brillante volume basato su una serie di conferenze intitolato What is Life?, il Nobel per la fisica Erwin Schrödinger delineò i tratti essenziali del modo in cui l’informazione genetica potrebbe essere immagazzinata, avvalendosi di una chiara similitudine con l’alfabeto Morse. Nel 1953 James Watson e Francis Crick pubblicarono il loro modello molecolare per la struttura del DNA, la celebre doppia elica, una delle icone della scienza contemporanea. Crick ha esplicitamente riconosciuto il suo debito intellettuale nei confronti del modello di Schrödinger. Nel 1962 Watson, Crick e Maurice Wilkins ricevettero congiuntamente il premio Nobel per la medicina per le loro scoperte concernenti la struttura molecolare degli acidi nucleici e il suo significato nel meccanismo di trasferimento dell’informazione negli organismi viventi. L’informazione è divenuta così una delle idee fondamentali della genetica.

Pag. 92.

## <a name="1447-53">[1447-53](#1447-53)</a>
>Il DNA contiene il codice genetico nel senso che contiene fisicamente i geni che codificano lo sviluppo dei fenotipi. In tal senso il DNA contiene informazioni genetiche come un cd può contenere un software. Ma è il codice genetico o meglio sono i geni stessi ad essere le informazioni. I geni non trasmettono le informazioni nello stesso senso in cui una radio trasmette un segnale. Operano con maggiore o minore successo e, come nel caso della ricetta di una torta, possono garantire solo in parte il risultato finale, poiché l’ambiente vi gioca un ruolo decisivo. Inoltre i geni non contengono informazioni, come una busta o un’e-mail, né le descrivono, come un progetto. Assomigliano piuttosto a performativi: “Prometto di arrivare alle venti” non descrive né contiene una promessa, ma fa qualcosa, vale a dire, realizza la promessa stessa per mezzo delle parole

Pag. 95.

## <a name="1473-77">[1473-77](#1473-77)</a>
>La relazione tra istruzioni (geni, programmi basati su comandi, ricette) e risultato è funzionale, causale e basata su leggi, ma non richiede che la semantica giochi alcun ruolo nel modo in cui l’hardware di un computer è costruito per eseguire il codice della macchina, che è scritto nello stile basato su comandi ed è il linguaggio originario del computer. Così, per dirlo con una formula celebre: “Nel codice genetico il mezzo (i geni) è il messaggio”. L’informazione biologica, nel senso predicativo del termine, è procedurale: è un’informazione per qualcosa, non a proposito di qualcosa.

Pag. 97.

## <a name="1492-99">[1492-99](#1492-99)</a>
>Senza modificazioni genetiche Giovanni non si sarebbe mai sviluppato. Lui e pressoché tutti gli altri animali (le spugne sono tra le rare eccezioni) appartengono ai cosiddetti bilaterali, organismi la cui forma del corpo è bilateralmente simmetrica. Reperti fossili mostrano che i bilaterali si sono probabilmente evoluti da un comune progenitore circa 550 milioni di anni fa. Con grande delusione di Giovanni, quel progenitore era un semplice verme tubo. Fortunatamente per lui, si trattava di una tipologia piuttosto speciale. Non è ancora del tutto chiaro quando e in che modo i bilaterali abbiano sviluppato un sistema nervoso, né come questo si sia ulteriormente evoluto in differenti gruppi di organismi. Ma, a partire da un momento cruciale di tale sviluppo, il progenitore di Giovanni ha acquisito un corpo segmentato, con un cordone nervoso, che aveva un allargamento, chiamato ganglio, per ogni segmento del corpo, e un ganglio alquanto più ampio alla fine del corpo, chiamato cervello. L’arma finale antientropica aveva visto la luce.

Pag. 98.

## <a name="1499-1505">[1499-1505](#1499-1505)</a>
>La vita biologica è una costante lotta contro l’entropia termodinamica. Un sistema vivente è un ente informazionale antientropico, vale a dire un oggetto informazionale, capace di dare luogo a interazioni procedurali (in quanto incorpora operazioni che processano informazioni), al fine di conservare la propria esistenza e/o di riprodurre se stesso (metabolismo). Persino gli organismi unicellulari ottengono informazioni dal proprio ambiente e reagiscono ad esse allo scopo di sopravvivere. Ma è soltanto con l’evoluzione di un sistema nervoso sofisticato capace di raccogliere, immagazzinare, processare e comunicare un’ingente massa di informazioni e di avvalersene con successo, che diventa possibile porre in essere e controllare una più ampia varietà di comportamenti antientropici.

Pag. 98.

## <a name="1569-70">[1569-70](#1569-70)</a>
>Uno dei grandi dilemmi informazionali concerne il modo in cui i segnali fisici, veicolati dal sistema nervoso, danno origine a un’informazione semantica di alto livello.

Pag. 103.

## <a name="1589-91">[1589-91](#1589-91)</a>
>Similmente in molti paesi è illecito negoziare strumenti finanziari di una società (ad esempio bond), laddove si abbia avuto accesso privilegiato a informazioni riservate di tale società, di regola acquisite lavorando per essa (per questo si parla in tal caso di insider trading).

Pag. 104.

## <a name="1598-1603">[1598-1603](#1598-1603)</a>
>Chiaramente quando parliamo di valore economico dell’informazione, l’informazione in questione è semantica. Per quanto questa sia matematicamente formulata e fisicamente implementata (ad esempio attraverso una telefonata, un’e-mail, un messaggio orale, un segnale radio, una formula chimica, una pagina web o una mappa) è il significato che l’informazione veicola a possedere un valore per gli agenti coinvolti, i quali assumono che essa sia corretta o veridica (si veda la Figura 7.1).

Pag. 105.

## <a name="1613-23">[1613-23](#1613-23)</a>
>Quando è considerata un bene commerciale, l’informazione ha tre proprietà principali che valgono a distinguerla da altri beni, come le auto o una pagnotta. In primo luogo, è un bene non rivale: il fatto che Giovanni detenga (consumi) l’informazione che la batteria è scarica non impedisce all’elettrauto di detenere (consumare) la medesima informazione allo stesso tempo. Questo non sarebbe possibile con una pagnotta. In secondo luogo l’informazione tende, di regola, ad essere un bene non esclusivo. Talune informazioni (come la proprietà intellettuale, i dati sensibili o riservati, i segreti militari) spesso sono protette, ma ciò richiede uno sforzo ulteriore proprio perché, normalmente, l’esclusività non è una proprietà naturale dell’informazione, la quale tende ad essere facilmente rivelata e condivisa. Al contrario, se il vicino di Giovanni gli presta i suoi cavi per la batteria non può usarli lui allo stesso tempo. Infine, dal momento in cui un’informazione è disponibile, il costo della sua riproduzione tende ad essere trascurabile (nessun costo marginale). Ciò naturalmente non è vero per beni come una pagnotta. Per tutte queste ragioni l’informazione può talora essere concepita come un bene pubblico, secondo una prospettiva che giustifica a sua volta l’apertura di biblioteche pubbliche o progetti come Wikipedia, a cui tutti hanno libero accesso.

Pag. 106.

## <a name="1897-99">[1897-99](#1897-99)</a>
>È noto che il “velo di ignoranza” di Rawls sfrutta esattamente questo aspetto dell’informazione come risorsa, allo scopo di sviluppare un approccio imparziale alla giustizia in termini di equità. Essere informato non è sempre una benedizione e, talora, può essere addirittura moralmente sbagliato o pericoloso.

Pag. 124.

## <a name="1926-27">[1926-27](#1926-27)</a>
>Il saggio di John Stuart Mill, Sulla libertà di pensiero e di parola, è un classico dell’etica dell’informazione come obiettivo.

Pag. 126.

## <a name="1972-76">[1972-76](#1972-76)</a>
>il nostro agente A (al pari di ogni altro ente) sarà un pacchetto discreto, indipendente, incapsulato, che contiene (I) le appropriate strutture di dati, che costituiscono la natura dell’ente in questione, vale a dire lo stato dell’oggetto, la sua identità unica e i suoi caratteri; e (II) un insieme di operazioni, funzioni o procedure, attivate da differenti interazioni e stimoli (ossia da messaggi ricevuti da altri oggetti o da modificazioni al proprio interno), che definiscono in modo corrispondente come l’oggetto si comporta o reagisce a tali sollecitazioni.

Pag. 129.

## <a name="1999-2022">[1999-2022](#1999-2022)</a>
>L’etica dell’informazione suggerisce l’idea che vi sia qualcosa di persino più elementare della vita, cioè l’essere, vale a dire l’esistenza e il fiorire di tutti gli enti e del loro ambiente globale, e qualcosa di ancora più fondamentale della sofferenza, cioè l’entropia. Quest’ultima non coincide in alcun modo con il concetto di entropia termodinamica, esaminato nel Capitolo 5, che definisce il livello di “rimescolamento” di un sistema. Nel contesto presente l’entropia fa riferimento ad ogni genere di distruzione, corruzione, dispersione e consunzione degli oggetti informazionali (e non semplicemente dell’informazione in quanto contenuto semantico), ossia ad ogni forma di impoverimento della realtà. L’etica dell’informazione fornisce dunque un vocabolario comune per comprendere l’intero dominio dell’essere in termini informazionali. L’etica dell’informazione ritiene che l’essere/informazione abbia un valore intrinseco e dà contenuto a tale posizione, riconoscendo che qualunque ente informazionale ha il diritto di persistere nel proprio status e il diritto di fiorire, ossia di migliorare e arricchire la propria esistenza ed essenza. In conseguenza di questi “diritti”, tale etica misura il dovere di ogni agente morale nei termini del contributo offerto alla crescita dell’infosfera e giudica ogni processo, azione o evento che condizioni negativamente l’intera infosfera (e non solamente un ente informazionale) come un incremento nel livello di entropia e pertanto come un’istanza del male. Nell’etica dell’informazione il discorso etico concerne ogni ente concepito in termini informazionali e dunque non soltanto le persone, la loro cultura, benessere e interazioni sociali, non soltanto gli animali, le piante e la loro vita naturale, ma anche tutto ciò che esiste, dai quadri ai libri, dalle stelle alle pietre e tutto ciò che può esistere o che esisterà, come le generazioni future; nonché tutto ciò che è stato e ora non è più, come i nostri avi o le antiche civiltà. L’etica dell’informazione è imparziale e universale poiché estende, fino al suo limite ultimo, il concetto di ciò che può qualificarsi come centro di interesse morale (non importa quanto tenue), che viene così a includere ogni istanza dell’essere, intesa in termini informazionali, che sia fisicamente implementata o meno. In tal senso l’etica dell’informazione ritiene che ogni ente, in quanto espressione dell’essere, abbia una propria dignità, costituita dal suo modo di esistenza ed essenza (l’insieme di tutte le proprietà elementari che lo costituiscono in ciò che è), che merita di essere rispettata (quantomeno in un senso minimo e controvertibile), e per questo sollevi pretese morali nei confronti degli agenti con cui interagisce, pretese che contribuiscono a limitare e a guidare le sue decisioni e i suoi comportamenti etici. Questo “principio di uguaglianza ontologica” implica che ogni forma di realtà (ogni istanza di informazione/essere), semplicemente per il fatto di essere ciò che è, gode di un pari diritto, minimo, iniziale, controvertibile a esistere e a svilupparsi in modo conforme alla propria natura.

Pag. 131.

## <a name="2044-56">[2044-56](#2044-56)</a>
>La forma logica di questo tipo di accordo può essere dunque usata come modello del trust ontico nei seguenti termini: •   i beni o il “corpus” sono rappresentati dal mondo, che include tutti gli agenti e i pazienti esistenti; •   i donatori sono costituiti da tutte le generazioni presenti e passate di agenti; •   i fiduciari sono costituiti da tutti gli agenti individuali presenti; •   i beneficiari sono rappresentati da tutti gli agenti e pazienti individuali presenti e futuri. Venendo a esistere, un agente è reso possibile grazie all’esistenza di altri enti. È pertanto legato a tutto ciò che già esiste sia involontariamente sia inevitabilmente. E dovrebbe esserlo anche “con cura”. Involontariamente poiché nessun agente ha voluto la propria esistenza, sebbene ogni agente possa, in teoria, liberarsene. Inevitabilmente perché il legame ontico può essere sciolto dall’agente soltanto a costo di cessare di esistere come agente. La vita morale non inizia con un atto di libertà ma può terminare con esso. Con cura poiché la partecipazione nella realtà da parte di qualsiasi ente, incluso un agente (vale a dire il fatto che ogni ente è espressione di ciò che esiste), offre un diritto all’esistenza e un invito (non un dovere) a rispettare e a prendersi cura degli altri enti.

Pag. 134.

## <a name="2066-80">[2066-80](#2066-80)</a>
>La bioetica e l’etica ambientale non riescono ad attingere a un livello di completa imparzialità poiché nutrono ancora pregiudizi nei confronti di ciò che è inanimato, privo di vita, intangibile o astratto (persino l’etica della terra, ad esempio, nutre un simile atteggiamento nei confronti della tecnologia e degli artefatti). Secondo la loro prospettiva soltanto ciò che è intuitivamente dotato di vita merita di essere considerato come centro di pretese morali, non importa quanto ridotte, cosicché un intero universo sfugge alla loro attenzione. Ora, è questo precisamente il limite fondamentale superato dall’etica dell’informazione, che sposta ulteriormente verso il basso la condizione minima a cui si deve attingere per qualificare qualcosa come centro di interesse morale: condizione minima che è costituita, dunque, da un fattore comune condiviso da ogni ente, vale a dire lo statuto informazionale. Dal momento che qualsiasi forma di essere è in ogni caso anche un corpo coerente di informazioni, dire che l’etica dell’informazione è infocentrica equivale a interpretarla, in modo corretto, come teoria ontocentrica. Da ciò deriva che tutti gli enti, in quanto oggetti informazionali, hanno un intrinseco valore morale, per quanto potenzialmente minimo e controvertibile, e pertanto possono essere considerati come pazienti morali, soggetti a un livello parimenti minimo di rispetto morale, compreso come attenzione disinteressata, capace di apprezzamento e di cura. Come ha sostenuto il filosofo Arne Naess,«tutti gli enti nella biosfera hanno un pari diritto a vivere e a fiorire». Non vi è alcuna buona ragione per non sposare una prospettiva ontocentrica più generale e inclusiva. Non soltanto gli oggetti inanimati ma anche quelli ideali, intangibili o intellettuali, hanno un livello minimo di valore morale, non importa quanto modesto, e pertanto hanno diritto ad essere rispettati.

Pag. 135.

## <a name="2080-89">[2080-89](#2080-89)</a>
>Vi è un celebre passaggio di una lettera di Einstein che riassume in modo perspicuo la visione prospettata dall’etica dell’informazione. Alcuni anni prima della sua morte, Albert Einstein ricevette una lettera da parte di una ragazza diciannovenne che piangeva la scomparsa della giovane sorella. La ragazza voleva sapere che cosa il celebre scienziato avrebbe potuto dire per consolarla. Il 4 marzo 1950 Einstein rispose alla ragazza: Un essere umano non è che una parte, limitata nel tempo e nello spazio, del tutto che chiamiamo l’universo. Egli sperimenta se stesso, i propri pensieri e sentimenti, come qualcosa di separato dal resto, una sorta di illusione ottica della propria coscienza. Questa illusione è come una prigione che ci costringe all’interno dei nostri desideri personali e del sentimento che nutriamo per poche persone a noi più vicine. Il nostro compito deve essere quello di liberarci di tale prigionia, ampliando il cerchio della compassione, per abbracciare tutta l’umanità e l’intera natura nella sua bellezza. Nessuno è capace di assolvere tale compito completamente, ma lottare per la sua realizzazione è di per sé parte della liberazione e la premessa di una più profonda sicurezza.

Pag. 136.

## <a name="2089-95">[2089-95](#2089-95)</a>
>Gli esponenti dell’ecologia profonda hanno già sostenuto che anche le cose inanimate posseggono un certo valore intrinseco. In un celebre articolo, lo storico Lynn Townsend White Jr. ha posto il seguente interrogativo: Le persone hanno obblighi etici nei confronti delle montagne? – e ha risposto che – Per pressoché tutti gli americani, assuefatti ormai a idee storicamente dominanti nel Cristianesimo […], questa domanda non ha alcun senso. Se è giunto il tempo in cui per un gruppo considerevole di persone questa domanda non appare più ridicola, significa che siamo alle soglie di un mutamento nelle strutture di valore che consentiranno di adottare misure idonee per fronteggiare la crescente crisi ecologica. Speriamo che vi sia ancora

Pag. 137.

## <a name="2095-2100">[2095-2100](#2095-2100)</a>
>Secondo l’etica dell’informazione, questa è la corretta prospettiva ecologica ed è perfettamente consistente con qualsiasi tradizione spirituale o religiosa (compresa quella giudaico-cristiana), per la quale l’intero universo è una creazione di Dio, abitata dal divino e donata all’umanità, che dovrebbe averne cura. L’etica dell’informazione traduce tutto questo in termini informazionali. Se qualsiasi cosa può qualificarsi come paziente morale, allora la sua natura può essere presa in considerazione da un agente morale A e contribuire a dare forma alle sue azioni, non importa se in modo più o meno accentuato.

Pag. 137.

## <a name="2123-30">[2123-30](#2123-30)</a>
>Se physis e techné possano essere riconciliate non è una domanda per cui esista una risposta predeterminata, che attende solo di essere scoperta. Si tratta piuttosto di un problema pratico, di cui occorre immaginare possibili soluzioni operative. Per dirlo con un’analogia, non ci stiamo chiedendo se due sostanze chimiche possano essere mescolate, ma se un matrimonio possa funzionare. Vi sono buone basi per una risposta positiva, purché si scelga di adottare l’attitudine giusta. È indubbio che un matrimonio felice tra physis e techné è di vitale importanza per il nostro futuro e pertanto merita il nostro impegno costante. Le società dell’informazione dipendono sempre di più dalla tecnologia per prosperare ma hanno bisogno al contempo di un ambiente salutare e naturale per crescere in modo armonioso. Proviamo a raffigurarci il mondo non di domani o del prossimo anno, ma tra un secolo o un millennio: il divorzio tra physis e techné sarebbe totalmente disastroso sia per il nostro benessere sia per quello del nostro habitat.

Pag. 139.

## <a name="2154-55">[2154-55](#2154-55)</a>
>La prospettiva più realistica e stimolante è quella che ritiene che il male morale sia inevitabile, con la conseguenza che il vero sforzo consiste allora nel limitarlo e nel controbilanciarlo con maggiore bene

Pag. 141.

## <a name="2156-63">[2156-63](#2156-63)</a>
>Le ICT possono aiutarci a combattere la distruzione, l’impoverimento, il vandalismo e lo spreco delle risorse naturali o umane, comprese quelle storiche e culturali, in modo tale da rivelarsi un prezioso alleato in ciò che ho definito altrove come ambientalismo sintetico o e-ambientalismo. Dovremmo resistere alla tentazione, propria della tradizione epistemologica greca, di considerare la techné come l’anello debole della conoscenza. Dovremmo parimenti resistere alla tendenza assolutistica a rifiutare ogni forma di bilanciamento tra una certa dose inevitabile di male e una maggiore quantità di bene o, ancora, alla tentazione moderna, reazionaria e metafisica, di separare nettamente naturalismo e costruzionismo, privilegiando il primo come unica autentica dimensione della vita umana. La sfida consiste nel riconciliare il nostro ruolo di organismi informazionali e agenti nella natura con quello di curatori della natura. La buona notizia è che si tratta di una sfida che possiamo vincere.

Pag. 141.
